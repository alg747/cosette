# Cosette’s source


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Setup

<details open class="code-fold">
<summary>Exported source</summary>

``` python
models = 'gpt-4o', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct'
```

</details>

``` python
model = models[0]
```

For examples, we’ll use GPT-4o.

## OpenAI SDK

``` python
cli = OpenAI().chat.completions
```

``` python
m = {'role': 'user', 'content': "I'm Jeremy"}
r = cli.create(messages=[m], model=model, max_tokens=100)
r
```

    ChatCompletion(id='chatcmpl-9RknY7Xgs53a0lVAFjGizFgC6ZUyG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1716401776, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_927397958d', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))

### Formatting output

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L36"
target="_blank" style="float:right; font-size:smaller">source</a>

### find_block

>      find_block (r:collections.abc.Mapping)

*Find the message in `r`.*

<table>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>r</td>
<td>Mapping</td>
<td>The message to look in</td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def find_block(r:abc.Mapping, # The message to look in
              ):
    "Find the message in `r`."
    m = nested_idx(r, 'choices', 0)
    if not m: return m
    if hasattr(m, 'message'): return m.message
    return m.delta
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L45"
target="_blank" style="float:right; font-size:smaller">source</a>

### contents

>      contents (r)

*Helper to get the contents from response `r`.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def contents(r):
    "Helper to get the contents from response `r`."
    blk = find_block(r)
    if not blk: return r
    if hasattr(blk, 'content'): return getattr(blk,'content')
    return blk
```

</details>

``` python
contents(r)
```

    'Hi Jeremy! How can I assist you today?'

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
def _repr_markdown_(self:ChatCompletion):
    det = '\n- '.join(f'{k}: {v}' for k,v in dict(self).items())
    res = contents(self)
    if not res: return f"- {det}"
    return f"""{contents(self)}

<details>

- {det}

</details>"""
```

</details>

``` python
r
```

Hi Jeremy! How can I assist you today?

<details>

- id: chatcmpl-9RknY7Xgs53a0lVAFjGizFgC6ZUyG
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘Hi Jeremy! How can I assist you
  today?’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716401776
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=10, prompt_tokens=9,
  total_tokens=19)

</details>

``` python
r.usage
```

    CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19)

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L67"
target="_blank" style="float:right; font-size:smaller">source</a>

### usage

>      usage (inp=0, out=0)

*Slightly more concise version of `CompletionUsage`.*

<table>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>inp</td>
<td>int</td>
<td>0</td>
<td>Number of prompt tokens</td>
</tr>
<tr class="even">
<td>out</td>
<td>int</td>
<td>0</td>
<td>Number of completion tokens</td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def usage(inp=0, # Number of prompt tokens
          out=0  # Number of completion tokens
         ):
    "Slightly more concise version of `CompletionUsage`."
    return CompletionUsage(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)
```

</details>

``` python
usage(5)
```

    CompletionUsage(completion_tokens=0, prompt_tokens=5, total_tokens=5)

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L75"
target="_blank" style="float:right; font-size:smaller">source</a>

### CompletionUsage.\_\_repr\_\_

>      CompletionUsage.__repr__ ()

*Return repr(self).*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
def __repr__(self:CompletionUsage): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'
```

</details>

``` python
r.usage
```

    In: 9; Out: 10; Total: 19

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L79"
target="_blank" style="float:right; font-size:smaller">source</a>

### CompletionUsage.\_\_add\_\_

>      CompletionUsage.__add__ (b)

*Add together each of `input_tokens` and `output_tokens`*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
def __add__(self:CompletionUsage, b):
    "Add together each of `input_tokens` and `output_tokens`"
    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)
```

</details>

``` python
r.usage+r.usage
```

    In: 18; Out: 20; Total: 38

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L84"
target="_blank" style="float:right; font-size:smaller">source</a>

### wrap_latex

>      wrap_latex (text, md=True)

*Replace OpenAI LaTeX codes with markdown-compatible ones*

### Creating messages

``` python
def mk_msg(content, role='user', **kwargs):
    "Helper to create a `dict` appropriate for a message. `kwargs` are added as key/value pairs to the message"
    if hasattr(content, 'content'): content,role = content.content,content.role
    if isinstance(content, ChatCompletion): return find_block(content)
    return dict(role=role, content=content, **kwargs)
```

``` python
prompt = "I'm Jeremy"
m = mk_msg(prompt)
r = cli.create(messages=[m], model=model, max_tokens=100)
r
```

Hi, Jeremy! How can I help you today?

<details>

- id: chatcmpl-9RknazPzTx3dhNW5ZmdHWO0lwQH8z
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘Hi, Jeremy! How can I help you
  today?’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716401778
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=11, prompt_tokens=9,
  total_tokens=20)

</details>

``` python
msgs = [mk_msg(prompt), mk_msg(r), mk_msg('I forgot my name. Can you remind me please?')]
msgs
```

    [{'role': 'user', 'content': "I'm Jeremy"},
     ChatCompletionMessage(content='Hi, Jeremy! How can I help you today?', role='assistant', function_call=None, tool_calls=None),
     {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]

``` python
cli.create(messages=msgs, model=model, max_tokens=200)
```

You mentioned earlier that your name is Jeremy. Is there anything else
you need help with?

<details>

- id: chatcmpl-9Rkna9bg5P5xZHalSUlwh2cpeaEEu
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘You mentioned earlier that your
  name is Jeremy. Is there anything else you need help with?’,
  role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716401778
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=18, prompt_tokens=39,
  total_tokens=57)

</details>

## Client

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L92"
target="_blank" style="float:right; font-size:smaller">source</a>

### Client

>      Client (model, cli=None)

*Basic LLM messages client.*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class Client:
    def __init__(self, model, cli=None):
        "Basic LLM messages client."
        self.model,self.use = model,usage(0,0)
        self.c = (cli or OpenAI()).chat.completions
```

</details>

``` python
c = Client(model)
c.use
```

    In: 0; Out: 0; Total: 0

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
def _r(self:Client, r:ChatCompletion):
    "Store the result of the message and accrue total usage."
    self.result = r
    if getattr(r,'usage',None): self.use += r.usage
    return r
```

</details>

``` python
c._r(r)
c.use
```

    In: 9; Out: 11; Total: 20

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L107"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_stream

>      get_stream (r)

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L115"
target="_blank" style="float:right; font-size:smaller">source</a>

### Client.\_\_call\_\_

>      Client.__call__ (msgs:list, sp:str='', maxtok=4096, stream:bool=False,
>                       frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN, fu
>                       nction_call:completion_create_params.FunctionCall|NotGiv
>                       en=NOT_GIVEN, functions:Iterable[completion_create_param
>                       s.Function]|NotGiven=NOT_GIVEN,
>                       logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,
>                       logprobs:Optional[bool]|NotGiven=NOT_GIVEN,
>                       max_tokens:Optional[int]|NotGiven=NOT_GIVEN,
>                       n:Optional[int]|NotGiven=NOT_GIVEN,
>                       parallel_tool_calls:bool|NotGiven=NOT_GIVEN,
>                       presence_penalty:Optional[float]|NotGiven=NOT_GIVEN, res
>                       ponse_format:completion_create_params.ResponseFormat|Not
>                       Given=NOT_GIVEN, seed:Optional[int]|NotGiven=NOT_GIVEN, 
>                       service_tier:"Optional[Literal['auto','default']]|NotGiv
>                       en"=NOT_GIVEN,
>                       stop:Union[Optional[str],List[str]]|NotGiven=NOT_GIVEN, 
>                       stream_options:Optional[ChatCompletionStreamOptionsParam
>                       ]|NotGiven=NOT_GIVEN,
>                       temperature:Optional[float]|NotGiven=NOT_GIVEN, tool_cho
>                       ice:ChatCompletionToolChoiceOptionParam|NotGiven=NOT_GIV
>                       EN, tools:Iterable[ChatCompletionToolParam]|NotGiven=NOT
>                       _GIVEN, top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,
>                       top_p:Optional[float]|NotGiven=NOT_GIVEN,
>                       user:str|NotGiven=NOT_GIVEN,
>                       extra_headers:Headers|None=None,
>                       extra_query:Query|None=None, extra_body:Body|None=None,
>                       timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)

*Make a call to LLM.*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>msgs</td>
<td>list</td>
<td></td>
<td>List of messages in the dialog</td>
</tr>
<tr class="even">
<td>sp</td>
<td>str</td>
<td></td>
<td>System prompt</td>
</tr>
<tr class="odd">
<td>maxtok</td>
<td>int</td>
<td>4096</td>
<td>Maximum tokens</td>
</tr>
<tr class="even">
<td>stream</td>
<td>bool</td>
<td>False</td>
<td>Stream response?</td>
</tr>
<tr class="odd">
<td>frequency_penalty</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>function_call</td>
<td>completion_create_params.FunctionCall | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>functions</td>
<td>Iterable[completion_create_params.Function] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>logit_bias</td>
<td>Optional[Dict[str, int]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>logprobs</td>
<td>Optional[bool] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>max_tokens</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>n</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>parallel_tool_calls</td>
<td>bool | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>presence_penalty</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>response_format</td>
<td>completion_create_params.ResponseFormat | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>seed</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>service_tier</td>
<td>Optional[Literal[‘auto’, ‘default’]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>stop</td>
<td>Union[Optional[str], List[str]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>stream_options</td>
<td>Optional[ChatCompletionStreamOptionsParam] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>temperature</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>tool_choice</td>
<td>ChatCompletionToolChoiceOptionParam | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>tools</td>
<td>Iterable[ChatCompletionToolParam] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>top_logprobs</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>top_p</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>user</td>
<td>str | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>extra_headers</td>
<td>Headers | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>extra_query</td>
<td>Query | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>extra_body</td>
<td>Body | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>timeout</td>
<td>float | httpx.Timeout | None | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
@delegates(Completions.create)
def __call__(self:Client,
             msgs:list, # List of messages in the dialog
             sp:str='', # System prompt
             maxtok=4096, # Maximum tokens
             stream:bool=False, # Stream response?
             **kwargs):
    "Make a call to LLM."
    if stream: kwargs['stream_options'] = {"include_usage": True}
    if sp: msgs = [mk_msg(sp, 'system')] + list(msgs)
    r = self.c.create(
        model=self.model, messages=msgs, max_tokens=maxtok, stream=stream, **kwargs)
    if not stream: return self._r(r)
    else: return get_stream(map(self._r, r))
```

</details>

``` python
msgs = [mk_msg('Hi')]
```

``` python
c(msgs)
```

Hello! How can I assist you today?

<details>

- id: chatcmpl-9RkncXq9HS6BFFZSWUZWZwMSM5B1t
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘Hello! How can I assist you
  today?’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716401780
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=9, prompt_tokens=8,
  total_tokens=17)

</details>

``` python
c.use
```

    In: 17; Out: 20; Total: 37

``` python
for o in c(msgs, stream=True): print(o, end='')
```

    Hello! How can I assist you today?

``` python
c.use
```

    In: 25; Out: 29; Total: 54

## Tool use

``` python
def sums(
    a:int,  # First thing to sum
    b:int # Second thing to sum
) -> int: # The sum of the inputs
    "Adds a + b."
    print(f"Finding the sum of {a} and {b}")
    return a + b
```

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L130"
target="_blank" style="float:right; font-size:smaller">source</a>

### mk_openai_func

>      mk_openai_func (f)

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L133"
target="_blank" style="float:right; font-size:smaller">source</a>

### mk_tool_choice

>      mk_tool_choice (f)

``` python
sysp = "You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum."
```

``` python
a,b = 604542,6458932
pr = f"What is {a}+{b}?"
tools=[mk_openai_func(sums)]
tool_choice=mk_tool_choice("sums")
```

``` python
msgs = [mk_msg(pr)]
r = c(msgs, sp=sysp, tools=tools)
r
```

- id: chatcmpl-9RknfnxfvYls5CN9xeKNCK1nCsOrW
- choices: \[Choice(finish_reason=‘tool_calls’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=None, role=‘assistant’,
  function_call=None,
  tool_calls=\[ChatCompletionMessageToolCall(id=‘call_lGvaujwxNi5M4Qow7q3MHVRK’,
  function=Function(arguments=‘{“a”:604542,“b”:6458932}’, name=‘sums’),
  type=‘function’)\]))\]
- created: 1716401783
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=21, prompt_tokens=104,
  total_tokens=125)

``` python
m = find_block(r)
m
```

    ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{"a":604542,"b":6458932}', name='sums'), type='function')])

``` python
tc = m.tool_calls
tc
```

    [ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{"a":604542,"b":6458932}', name='sums'), type='function')]

``` python
func = tc[0].function
func
```

    Function(arguments='{"a":604542,"b":6458932}', name='sums')

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L141"
target="_blank" style="float:right; font-size:smaller">source</a>

### call_func

>      call_func
>                 (fc:openai.types.chat.chat_completion_message_tool_call.Functi
>                 on, ns:Optional[collections.abc.Mapping]=None,
>                 obj:Optional=None)

*Call the function in the tool response `tr`, using namespace `ns`.*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fc</td>
<td>Function</td>
<td></td>
<td>Function block from message</td>
</tr>
<tr class="even">
<td>ns</td>
<td>Optional</td>
<td>None</td>
<td>Namespace to search for tools, defaults to
<code>globals()</code></td>
</tr>
<tr class="odd">
<td>obj</td>
<td>Optional</td>
<td>None</td>
<td>Object to search for tools</td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _mk_ns(*funcs:list[callable]) -> dict[str,callable]:
    "Create a `dict` of name to function in `funcs`, to use as a namespace"
    return {f.__name__:f for f in funcs}
```

</details>
<details open class="code-fold">
<summary>Exported source</summary>

``` python
def call_func(fc:types.chat.chat_completion_message_tool_call.Function, # Function block from message
              ns:Optional[abc.Mapping]=None, # Namespace to search for tools, defaults to `globals()`
              obj:Optional=None # Object to search for tools
             ):
    "Call the function in the tool response `tr`, using namespace `ns`."
    if ns is None: ns=globals()
    if not isinstance(ns, abc.Mapping): ns = _mk_ns(*ns)
    func = getattr(obj, fc.name, None)
    if not func: func = ns[fc.name]
    return func(**ast.literal_eval(fc.arguments))
```

</details>

``` python
ns = _mk_ns(sums)
res = call_func(func, ns=ns)
res
```

    Finding the sum of 604542 and 6458932

    7063474

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L153"
target="_blank" style="float:right; font-size:smaller">source</a>

### mk_toolres

>      mk_toolres (r:collections.abc.Mapping,
>                  ns:Optional[collections.abc.Mapping]=None, obj:Optional=None)

*Create a `tool_result` message from response `r`.*

<table>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>r</td>
<td>Mapping</td>
<td></td>
<td>Tool use request response</td>
</tr>
<tr class="even">
<td>ns</td>
<td>Optional</td>
<td>None</td>
<td>Namespace to search for tools</td>
</tr>
<tr class="odd">
<td>obj</td>
<td>Optional</td>
<td>None</td>
<td>Class to search for tools</td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def mk_toolres(
    r:abc.Mapping, # Tool use request response
    ns:Optional[abc.Mapping]=None, # Namespace to search for tools
    obj:Optional=None # Class to search for tools
    ):
    "Create a `tool_result` message from response `r`."
    r = mk_msg(r)
    tcs = getattr(r, 'tool_calls', [])
    res = [r]
    for tc in (tcs or []):
        func = tc.function
        cts = str(call_func(func, ns=ns, obj=obj))
        res.append(mk_msg(str(cts), 'tool', tool_call_id=tc.id, name=func.name))
    return res
```

</details>

``` python
tr = mk_toolres(r, ns=ns)
tr
```

    Finding the sum of 604542 and 6458932

    [ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{"a":604542,"b":6458932}', name='sums'), type='function')]),
     {'role': 'tool',
      'content': '7063474',
      'tool_call_id': 'call_lGvaujwxNi5M4Qow7q3MHVRK',
      'name': 'sums'}]

``` python
msgs += tr
```

``` python
res = c(msgs, sp=sysp, tools=tools)
res
```

The sum of ( 604542 ) and ( 6458932 ) is ( 7063474 ).

<details>

- id: chatcmpl-9RkngEFS4HLusmCmTu4HjoGjcS0mD
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The sum of \\ 604542 \\ and \\
  6458932 \\ is \\ 7063474 \\.’, role=‘assistant’, function_call=None,
  tool_calls=None))\]
- created: 1716401784
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=26, prompt_tokens=136,
  total_tokens=162)

</details>

``` python
class Dummy:
    def sums(
        self,
        a:int,  # First thing to sum
        b:int=1 # Second thing to sum
    ) -> int: # The sum of the inputs
        "Adds a + b."
        print(f"Finding the sum of {a} and {b}")
        return a + b
```

``` python
tools = [mk_openai_func(Dummy.sums)]

o = Dummy()
msgs = mk_toolres("I'm Jeremy")
r = c(msgs, sp=sysp, tools=tools)
msgs += mk_toolres(r, obj=o)
res = c(msgs, sp=sysp, tools=tools)
res
```

Hi Jeremy! How can I assist you today?

<details>

- id: chatcmpl-9RknjIS83HvoyhsyP02jOPArmI1Kd
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘Hi Jeremy! How can I assist you
  today?’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716401787
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=11, prompt_tokens=116,
  total_tokens=127)

</details>

``` python
msgs
```

    [{'role': 'user', 'content': "I'm Jeremy"},
     ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)]

``` python
tools = [mk_openai_func(Dummy.sums)]

o = Dummy()
msgs = mk_toolres(pr)
r = c(msgs, sp=sysp, tools=tools)
msgs += mk_toolres(r, obj=o)
res = c(msgs, sp=sysp, tools=tools)
res
```

    Finding the sum of 604542 and 6458932

The sum of 604542 and 6458932 is 7063474.

<details>

- id: chatcmpl-9RknkRB7hkdbZ0O9nQUDK2pULVYP4
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The sum of 604542 and 6458932
  is 7063474.’, role=‘assistant’, function_call=None,
  tool_calls=None))\]
- created: 1716401788
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=18, prompt_tokens=142,
  total_tokens=160)

</details>

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L171"
target="_blank" style="float:right; font-size:smaller">source</a>

### mock_tooluse

>      mock_tooluse (name:str, res, **kwargs)

<table>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>name</td>
<td>str</td>
<td>The name of the called function</td>
</tr>
<tr class="even">
<td>res</td>
<td></td>
<td>The result of calling the function</td>
</tr>
<tr class="odd">
<td>kwargs</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _mock_id(): return 'call_' + ''.join(choices(ascii_letters+digits, k=24))

def mock_tooluse(name:str, # The name of the called function
                 res,  # The result of calling the function
                 **kwargs): # The arguments to the function
    ""
    id = _mock_id()
    func = dict(arguments=json.dumps(kwargs), name=name)
    tc = dict(id=id, function=func, type='function')
    req = dict(content=None, role='assistant', tool_calls=[tc])
    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)
    return [req,resp]
```

</details>

This function mocks the messages needed to implement tool use, for
situations where you want to insert tool use messages into a dialog
without actually calling into the model.

``` python
tu = mk_tooluse(name='sums', res=7063474, a=604542, b=6458932)
r = c([mk_msg(pr)]+tu, tools=tools)
r
```

The sum of 604542 and 6458932 is 7,063,474.

<details>

- id: chatcmpl-9Rkv7ncClEoHmJjrHrOb1sj7VZ269
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The sum of 604542 and 6458932
  is 7,063,474.’, role=‘assistant’, function_call=None,
  tool_calls=None))\]
- created: 1716402245
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=20, prompt_tokens=121,
  total_tokens=141)

</details>

## Chat

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L183"
target="_blank" style="float:right; font-size:smaller">source</a>

### Chat

>      Chat (model:Optional[str]=None, cli:Optional[__main__.Client]=None,
>            sp='', tools:Optional[list]=None, tool_choice:Optional[str]=None)

*OpenAI chat client.*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>model</td>
<td>Optional</td>
<td>None</td>
<td>Model to use (leave empty if passing <code>cli</code>)</td>
</tr>
<tr class="even">
<td>cli</td>
<td>Optional</td>
<td>None</td>
<td>Client to use (leave empty if passing <code>model</code>)</td>
</tr>
<tr class="odd">
<td>sp</td>
<td>str</td>
<td></td>
<td>Optional system prompt</td>
</tr>
<tr class="even">
<td>tools</td>
<td>Optional</td>
<td>None</td>
<td>List of tools to make available</td>
</tr>
<tr class="odd">
<td>tool_choice</td>
<td>Optional</td>
<td>None</td>
<td>Forced tool choice</td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
class Chat:
    def __init__(self,
                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)
                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)
                 sp='', # Optional system prompt
                 tools:Optional[list]=None,  # List of tools to make available
                 tool_choice:Optional[str]=None): # Forced tool choice
        "OpenAI chat client."
        assert model or cli
        self.c = (cli or Client(model))
        self.h,self.sp,self.tools,self.tool_choice = [],sp,tools,tool_choice
    
    @property
    def use(self): return self.c.use
```

</details>

``` python
sp = "Never mention what tools you use."
chat = Chat(model, sp=sp)
chat.c.use, chat.h
```

    (In: 0; Out: 0; Total: 0, [])

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L201"
target="_blank" style="float:right; font-size:smaller">source</a>

### Chat.\_\_call\_\_

>      Chat.__call__ (pr=None, stream:bool=False,
>                     frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN, func
>                     tion_call:completion_create_params.FunctionCall|NotGiven=N
>                     OT_GIVEN, functions:Iterable[completion_create_params.Func
>                     tion]|NotGiven=NOT_GIVEN,
>                     logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,
>                     logprobs:Optional[bool]|NotGiven=NOT_GIVEN,
>                     max_tokens:Optional[int]|NotGiven=NOT_GIVEN,
>                     n:Optional[int]|NotGiven=NOT_GIVEN,
>                     parallel_tool_calls:bool|NotGiven=NOT_GIVEN,
>                     presence_penalty:Optional[float]|NotGiven=NOT_GIVEN, respo
>                     nse_format:completion_create_params.ResponseFormat|NotGive
>                     n=NOT_GIVEN, seed:Optional[int]|NotGiven=NOT_GIVEN, servic
>                     e_tier:"Optional[Literal['auto','default']]|NotGiven"=NOT_
>                     GIVEN,
>                     stop:Union[Optional[str],List[str]]|NotGiven=NOT_GIVEN, st
>                     ream_options:Optional[ChatCompletionStreamOptionsParam]|No
>                     tGiven=NOT_GIVEN,
>                     temperature:Optional[float]|NotGiven=NOT_GIVEN, tool_choic
>                     e:ChatCompletionToolChoiceOptionParam|NotGiven=NOT_GIVEN, 
>                     tools:Iterable[ChatCompletionToolParam]|NotGiven=NOT_GIVEN
>                     , top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,
>                     top_p:Optional[float]|NotGiven=NOT_GIVEN,
>                     user:str|NotGiven=NOT_GIVEN,
>                     extra_headers:Headers|None=None,
>                     extra_query:Query|None=None, extra_body:Body|None=None,
>                     timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)

*Add prompt `pr` to dialog and get a response*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pr</td>
<td>NoneType</td>
<td>None</td>
<td>Prompt / message</td>
</tr>
<tr class="even">
<td>stream</td>
<td>bool</td>
<td>False</td>
<td>Stream response?</td>
</tr>
<tr class="odd">
<td>frequency_penalty</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>function_call</td>
<td>completion_create_params.FunctionCall | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>functions</td>
<td>Iterable[completion_create_params.Function] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>logit_bias</td>
<td>Optional[Dict[str, int]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>logprobs</td>
<td>Optional[bool] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>max_tokens</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>n</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>parallel_tool_calls</td>
<td>bool | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>presence_penalty</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>response_format</td>
<td>completion_create_params.ResponseFormat | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>seed</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>service_tier</td>
<td>Optional[Literal[‘auto’, ‘default’]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>stop</td>
<td>Union[Optional[str], List[str]] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>stream_options</td>
<td>Optional[ChatCompletionStreamOptionsParam] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>temperature</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>tool_choice</td>
<td>ChatCompletionToolChoiceOptionParam | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>tools</td>
<td>Iterable[ChatCompletionToolParam] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>top_logprobs</td>
<td>Optional[int] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>top_p</td>
<td>Optional[float] | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="even">
<td>user</td>
<td>str | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
<tr class="odd">
<td>extra_headers</td>
<td>Headers | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>extra_query</td>
<td>Query | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>extra_body</td>
<td>Body | None</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>timeout</td>
<td>float | httpx.Timeout | None | NotGiven</td>
<td>NOT_GIVEN</td>
<td></td>
</tr>
</tbody>
</table>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
@patch
@delegates(Completions.create)
def __call__(self:Chat,
             pr=None,  # Prompt / message
             stream:bool=False, # Stream response?
             **kwargs):
    "Add prompt `pr` to dialog and get a response"
    if isinstance(pr,str): pr = pr.strip()
    if pr: self.h.append(mk_msg(pr))
    if self.tools: kwargs['tools'] = [mk_openai_func(o) for o in self.tools]
    if self.tool_choice: kwargs['tool_choice'] = mk_tool_choice(tool_choice)
    res = self.c(self.h, sp=self.sp, stream=stream, **kwargs)
    self.h += mk_toolres(res, ns=self.tools, obj=self)
    return res
```

</details>

``` python
chat("I'm Jeremy")
chat("What's my name?")
```

Your name is Jeremy. How can I help you today?

<details>

- id: chatcmpl-9R81BjEI4V2NzVnAcC3wsGs5zVzhx
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘Your name is Jeremy. How can I
  help you today?’, role=‘assistant’, function_call=None,
  tool_calls=None))\]
- created: 1716252705
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=12, prompt_tokens=47,
  total_tokens=59)

</details>

``` python
chat = Chat(model, sp=sp)
for o in chat("I'm Jeremy", stream=True):
    o = contents(o)
    if o and isinstance(o, str): print(o, end='')
```

    Hi Jeremy! How can I assist you today?

### Chat tool use

``` python
pr = f"What is {a}+{b}?"
pr
```

    'What is 604542+6458932?'

``` python
chat = Chat(model, sp=sp, tools=[sums])
r = chat(pr)
r
```

    Finding the sum of 604542 and 6458932

- id: chatcmpl-9R81DDZbcRBWExEW7NU29xar0na3H
- choices: \[Choice(finish_reason=‘tool_calls’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=None, role=‘assistant’,
  function_call=None,
  tool_calls=\[ChatCompletionMessageToolCall(id=‘call_qwCoELHOZa325UyxdKRAbP8P’,
  function=Function(arguments=‘{“a”:604542,“b”:6458932}’, name=‘sums’),
  type=‘function’)\]))\]
- created: 1716252707
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=21, prompt_tokens=90,
  total_tokens=111)

``` python
chat()
```

The result of 604542 + 6458932 is 7063474.

<details>

- id: chatcmpl-9R81DrU0V9VmVO9vKA4vtDMXgqZRq
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The result of 604542 + 6458932
  is 7063474.’, role=‘assistant’, function_call=None,
  tool_calls=None))\]
- created: 1716252707
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_729ea513f7
- usage: CompletionUsage(completion_tokens=18, prompt_tokens=122,
  total_tokens=140)

</details>

## Images

As everyone knows, when testing image APIs you have to use a cute puppy.

``` python
# Image is Cute_dog.jpg from Wikimedia
fn = Path('samples/puppy.jpg')
display.Image(filename=fn, width=200)
```

<img src="00_core_files/figure-commonmark/cell-76-output-1.jpeg"
width="200" />

``` python
img = fn.read_bytes()
```

``` js
{
  "type": "image_url",
  "image_url": {
    "url": f"data:image/jpeg;base64,{base64_image}"
  }
}
```

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L215"
target="_blank" style="float:right; font-size:smaller">source</a>

### img_msg

>      img_msg (data:bytes)

*Convert image `data` into an encoded `dict`*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def img_msg(data:bytes)->dict:
    "Convert image `data` into an encoded `dict`"
    img = base64.b64encode(data).decode("utf-8")
    mtype = mimetypes.types_map['.'+imghdr.what(None, h=data)]
    r = {'url': f"data:{mtype};base64,{img}"}
    return {'type': "image_url", "image_url": r}
```

</details>

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L223"
target="_blank" style="float:right; font-size:smaller">source</a>

### text_msg

>      text_msg (s:str)

*Convert `s` to a text message*

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def text_msg(s:str)->dict:
    "Convert `s` to a text message"
    return {"type": "text", "text": s}
```

</details>

``` python
q = "In brief, what color flowers are in this image?"
msg = mk_msg([img_msg(img), text_msg(q)])
```

``` python
c([msg])
```

The flowers in the image are purple.

<details>

- id: chatcmpl-9R7157vVHkk0ZWqLl1bwZkfJPe21H
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The flowers in the image are
  purple.’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716248855
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=8, prompt_tokens=273,
  total_tokens=281)

</details>
<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _mk_content(src):
    "Create appropriate content data structure based on type of content"
    if isinstance(src,str): return text_msg(src)
    if isinstance(src,bytes): return img_msg(src)
    return src
```

</details>

There’s not need to manually choose the type of message, since we figure
that out from the data of the source data.

``` python
_mk_content('Hi')
```

    {'type': 'text', 'text': 'Hi'}

------------------------------------------------------------------------

<a
href="https://github.com/AnswerDotAI/cosette/blob/main/cosette/core.py#L235"
target="_blank" style="float:right; font-size:smaller">source</a>

### mk_msg

>      mk_msg (content, role='user', **kwargs)

*Helper to create a `dict` appropriate for a message. `kwargs` are added
as key/value pairs to the message*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>content</td>
<td></td>
<td></td>
<td>A string, list, or dict containing the contents of the message</td>
</tr>
<tr class="even">
<td>role</td>
<td>str</td>
<td>user</td>
<td>Must be ‘user’ or ‘assistant’</td>
</tr>
<tr class="odd">
<td>kwargs</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

``` python
c([mk_msg([img, q])])
```

The flowers in this image are purple.

<details>

- id: chatcmpl-9R76D0a9kD3Hy2IuBScq0VhqyFcaz
- choices: \[Choice(finish_reason=‘stop’, index=0, logprobs=None,
  message=ChatCompletionMessage(content=‘The flowers in this image are
  purple.’, role=‘assistant’, function_call=None, tool_calls=None))\]
- created: 1716249173
- model: gpt-4o-2024-05-13
- object: chat.completion
- system_fingerprint: fp_927397958d
- usage: CompletionUsage(completion_tokens=8, prompt_tokens=273,
  total_tokens=281)

</details>
