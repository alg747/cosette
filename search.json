[
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "",
    "section": "0.0.2",
    "text": "0.0.2\n\nInitial release"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Cosette’s source",
    "section": "",
    "text": "Exported source\nmodels = 'gpt-4o', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct'\n\n\n\nmodel = models[0]\n\nFor examples, we’ll use GPT-4o.",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Cosette’s source",
    "section": "",
    "text": "Exported source\nmodels = 'gpt-4o', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct'\n\n\n\nmodel = models[0]\n\nFor examples, we’ll use GPT-4o.",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#openai-sdk",
    "href": "core.html#openai-sdk",
    "title": "Cosette’s source",
    "section": "OpenAI SDK",
    "text": "OpenAI SDK\n\ncli = OpenAI().chat.completions\n\n\nm = {'role': 'user', 'content': \"I'm Jeremy\"}\nr = cli.create(messages=[m], model=model, max_tokens=100)\nr\n\nChatCompletion(id='chatcmpl-9RknY7Xgs53a0lVAFjGizFgC6ZUyG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1716401776, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_927397958d', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n\n\n\nFormatting output\n\nsource\n\n\nfind_block\n\n find_block (r:collections.abc.Mapping)\n\nFind the message in r.\n\n\n\n\nType\nDetails\n\n\n\n\nr\nMapping\nThe message to look in\n\n\n\n\n\nExported source\ndef find_block(r:abc.Mapping, # The message to look in\n              ):\n    \"Find the message in `r`.\"\n    m = nested_idx(r, 'choices', 0)\n    if not m: return m\n    if hasattr(m, 'message'): return m.message\n    return m.delta\n\n\n\nsource\n\n\ncontents\n\n contents (r)\n\nHelper to get the contents from response r.\n\n\nExported source\ndef contents(r):\n    \"Helper to get the contents from response `r`.\"\n    blk = find_block(r)\n    if not blk: return r\n    if hasattr(blk, 'content'): return getattr(blk,'content')\n    return blk\n\n\n\ncontents(r)\n\n'Hi Jeremy! How can I assist you today?'\n\n\n\n\nExported source\n@patch\ndef _repr_markdown_(self:ChatCompletion):\n    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n    res = contents(self)\n    if not res: return f\"- {det}\"\n    return f\"\"\"{contents(self)}\n\n&lt;details&gt;\n\n- {det}\n\n&lt;/details&gt;\"\"\"\n\n\n\nr\n\nHi Jeremy! How can I assist you today?\n\n\nid: chatcmpl-9RknY7Xgs53a0lVAFjGizFgC6ZUyG\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Hi Jeremy! How can I assist you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401776\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19)\n\n\n\n\n\nr.usage\n\nCompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19)\n\n\n\nsource\n\n\nusage\n\n usage (inp=0, out=0)\n\nSlightly more concise version of CompletionUsage.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\nint\n0\nNumber of prompt tokens\n\n\nout\nint\n0\nNumber of completion tokens\n\n\n\n\n\nExported source\ndef usage(inp=0, # Number of prompt tokens\n          out=0  # Number of completion tokens\n         ):\n    \"Slightly more concise version of `CompletionUsage`.\"\n    return CompletionUsage(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)\n\n\n\nusage(5)\n\nCompletionUsage(completion_tokens=0, prompt_tokens=5, total_tokens=5)\n\n\n\nsource\n\n\nCompletionUsage.__repr__\n\n CompletionUsage.__repr__ ()\n\nReturn repr(self).\n\n\nExported source\n@patch\ndef __repr__(self:CompletionUsage): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'\n\n\n\nr.usage\n\nIn: 9; Out: 10; Total: 19\n\n\n\nsource\n\n\nCompletionUsage.__add__\n\n CompletionUsage.__add__ (b)\n\nAdd together each of input_tokens and output_tokens\n\n\nExported source\n@patch\ndef __add__(self:CompletionUsage, b):\n    \"Add together each of `input_tokens` and `output_tokens`\"\n    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)\n\n\n\nr.usage+r.usage\n\nIn: 18; Out: 20; Total: 38\n\n\n\nsource\n\n\nwrap_latex\n\n wrap_latex (text, md=True)\n\nReplace OpenAI LaTeX codes with markdown-compatible ones\n\n\nCreating messages\n\ndef mk_msg(content, role='user', **kwargs):\n    \"Helper to create a `dict` appropriate for a message. `kwargs` are added as key/value pairs to the message\"\n    if hasattr(content, 'content'): content,role = content.content,content.role\n    if isinstance(content, ChatCompletion): return find_block(content)\n    return dict(role=role, content=content, **kwargs)\n\n\nprompt = \"I'm Jeremy\"\nm = mk_msg(prompt)\nr = cli.create(messages=[m], model=model, max_tokens=100)\nr\n\nHi, Jeremy! How can I help you today?\n\n\nid: chatcmpl-9RknazPzTx3dhNW5ZmdHWO0lwQH8z\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Hi, Jeremy! How can I help you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401778\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=11, prompt_tokens=9, total_tokens=20)\n\n\n\n\n\nmsgs = [mk_msg(prompt), mk_msg(r), mk_msg('I forgot my name. Can you remind me please?')]\nmsgs\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ChatCompletionMessage(content='Hi, Jeremy! How can I help you today?', role='assistant', function_call=None, tool_calls=None),\n {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]\n\n\n\ncli.create(messages=msgs, model=model, max_tokens=200)\n\nYou mentioned earlier that your name is Jeremy. Is there anything else you need help with?\n\n\nid: chatcmpl-9Rkna9bg5P5xZHalSUlwh2cpeaEEu\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘You mentioned earlier that your name is Jeremy. Is there anything else you need help with?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401778\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=18, prompt_tokens=39, total_tokens=57)",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#client",
    "href": "core.html#client",
    "title": "Cosette’s source",
    "section": "Client",
    "text": "Client\n\nsource\n\nClient\n\n Client (model, cli=None)\n\nBasic LLM messages client.\n\n\nExported source\nclass Client:\n    def __init__(self, model, cli=None):\n        \"Basic LLM messages client.\"\n        self.model,self.use = model,usage(0,0)\n        self.c = (cli or OpenAI()).chat.completions\n\n\n\nc = Client(model)\nc.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nExported source\n@patch\ndef _r(self:Client, r:ChatCompletion):\n    \"Store the result of the message and accrue total usage.\"\n    self.result = r\n    if getattr(r,'usage',None): self.use += r.usage\n    return r\n\n\n\nc._r(r)\nc.use\n\nIn: 9; Out: 11; Total: 20\n\n\n\nsource\n\n\nget_stream\n\n get_stream (r)\n\n\nsource\n\n\nClient.__call__\n\n Client.__call__ (msgs:list, sp:str='', maxtok=4096, stream:bool=False,\n                  frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN, fu\n                  nction_call:completion_create_params.FunctionCall|NotGiv\n                  en=NOT_GIVEN, functions:Iterable[completion_create_param\n                  s.Function]|NotGiven=NOT_GIVEN,\n                  logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n                  logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n                  max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n                  n:Optional[int]|NotGiven=NOT_GIVEN,\n                  parallel_tool_calls:bool|NotGiven=NOT_GIVEN,\n                  presence_penalty:Optional[float]|NotGiven=NOT_GIVEN, res\n                  ponse_format:completion_create_params.ResponseFormat|Not\n                  Given=NOT_GIVEN, seed:Optional[int]|NotGiven=NOT_GIVEN, \n                  service_tier:\"Optional[Literal['auto','default']]|NotGiv\n                  en\"=NOT_GIVEN,\n                  stop:Union[Optional[str],List[str]]|NotGiven=NOT_GIVEN, \n                  stream_options:Optional[ChatCompletionStreamOptionsParam\n                  ]|NotGiven=NOT_GIVEN,\n                  temperature:Optional[float]|NotGiven=NOT_GIVEN, tool_cho\n                  ice:ChatCompletionToolChoiceOptionParam|NotGiven=NOT_GIV\n                  EN, tools:Iterable[ChatCompletionToolParam]|NotGiven=NOT\n                  _GIVEN, top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n                  top_p:Optional[float]|NotGiven=NOT_GIVEN,\n                  user:str|NotGiven=NOT_GIVEN,\n                  extra_headers:Headers|None=None,\n                  extra_query:Query|None=None, extra_body:Body|None=None,\n                  timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nMake a call to LLM.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nList of messages in the dialog\n\n\nsp\nstr\n\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\nfrequency_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nfunction_call\ncompletion_create_params.FunctionCall | NotGiven\nNOT_GIVEN\n\n\n\nfunctions\nIterable[completion_create_params.Function] | NotGiven\nNOT_GIVEN\n\n\n\nlogit_bias\nOptional[Dict[str, int]] | NotGiven\nNOT_GIVEN\n\n\n\nlogprobs\nOptional[bool] | NotGiven\nNOT_GIVEN\n\n\n\nmax_tokens\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nn\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nparallel_tool_calls\nbool | NotGiven\nNOT_GIVEN\n\n\n\npresence_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nresponse_format\ncompletion_create_params.ResponseFormat | NotGiven\nNOT_GIVEN\n\n\n\nseed\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’]] | NotGiven\nNOT_GIVEN\n\n\n\nstop\nUnion[Optional[str], List[str]] | NotGiven\nNOT_GIVEN\n\n\n\nstream_options\nOptional[ChatCompletionStreamOptionsParam] | NotGiven\nNOT_GIVEN\n\n\n\ntemperature\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\ntool_choice\nChatCompletionToolChoiceOptionParam | NotGiven\nNOT_GIVEN\n\n\n\ntools\nIterable[ChatCompletionToolParam] | NotGiven\nNOT_GIVEN\n\n\n\ntop_logprobs\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\ntop_p\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nuser\nstr | NotGiven\nNOT_GIVEN\n\n\n\nextra_headers\nHeaders | None\nNone\n\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Completions.create)\ndef __call__(self:Client,\n             msgs:list, # List of messages in the dialog\n             sp:str='', # System prompt\n             maxtok=4096, # Maximum tokens\n             stream:bool=False, # Stream response?\n             **kwargs):\n    \"Make a call to LLM.\"\n    if stream: kwargs['stream_options'] = {\"include_usage\": True}\n    if sp: msgs = [mk_msg(sp, 'system')] + list(msgs)\n    r = self.c.create(\n        model=self.model, messages=msgs, max_tokens=maxtok, stream=stream, **kwargs)\n    if not stream: return self._r(r)\n    else: return get_stream(map(self._r, r))\n\n\n\nmsgs = [mk_msg('Hi')]\n\n\nc(msgs)\n\nHello! How can I assist you today?\n\n\nid: chatcmpl-9RkncXq9HS6BFFZSWUZWZwMSM5B1t\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Hello! How can I assist you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401780\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=9, prompt_tokens=8, total_tokens=17)\n\n\n\n\n\nc.use\n\nIn: 17; Out: 20; Total: 37\n\n\n\nfor o in c(msgs, stream=True): print(o, end='')\n\nHello! How can I assist you today?\n\n\n\nc.use\n\nIn: 25; Out: 29; Total: 54",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#tool-use",
    "href": "core.html#tool-use",
    "title": "Cosette’s source",
    "section": "Tool use",
    "text": "Tool use\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\nsource\n\nmk_openai_func\n\n mk_openai_func (f)\n\n\nsource\n\n\nmk_tool_choice\n\n mk_tool_choice (f)\n\n\nsysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\ntools=[mk_openai_func(sums)]\ntool_choice=mk_tool_choice(\"sums\")\n\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools)\nr\n\n\nid: chatcmpl-9RknfnxfvYls5CN9xeKNCK1nCsOrW\nchoices: [Choice(finish_reason=‘tool_calls’, index=0, logprobs=None, message=ChatCompletionMessage(content=None, role=‘assistant’, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=‘call_lGvaujwxNi5M4Qow7q3MHVRK’, function=Function(arguments=‘{“a”:604542,“b”:6458932}’, name=‘sums’), type=‘function’)]))]\ncreated: 1716401783\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=21, prompt_tokens=104, total_tokens=125)\n\n\n\n\nm = find_block(r)\nm\n\nChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')])\n\n\n\ntc = m.tool_calls\ntc\n\n[ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]\n\n\n\nfunc = tc[0].function\nfunc\n\nFunction(arguments='{\"a\":604542,\"b\":6458932}', name='sums')\n\n\n\nsource\n\n\ncall_func\n\n call_func\n            (fc:openai.types.chat.chat_completion_message_tool_call.Functi\n            on, ns:Optional[collections.abc.Mapping]=None,\n            obj:Optional=None)\n\nCall the function in the tool response tr, using namespace ns.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfc\nFunction\n\nFunction block from message\n\n\nns\nOptional\nNone\nNamespace to search for tools, defaults to globals()\n\n\nobj\nOptional\nNone\nObject to search for tools\n\n\n\n\n\nExported source\ndef _mk_ns(*funcs:list[callable]) -&gt; dict[str,callable]:\n    \"Create a `dict` of name to function in `funcs`, to use as a namespace\"\n    return {f.__name__:f for f in funcs}\n\n\n\n\nExported source\ndef call_func(fc:types.chat.chat_completion_message_tool_call.Function, # Function block from message\n              ns:Optional[abc.Mapping]=None, # Namespace to search for tools, defaults to `globals()`\n              obj:Optional=None # Object to search for tools\n             ):\n    \"Call the function in the tool response `tr`, using namespace `ns`.\"\n    if ns is None: ns=globals()\n    if not isinstance(ns, abc.Mapping): ns = _mk_ns(*ns)\n    func = getattr(obj, fc.name, None)\n    if not func: func = ns[fc.name]\n    return func(**ast.literal_eval(fc.arguments))\n\n\n\nns = _mk_ns(sums)\nres = call_func(func, ns=ns)\nres\n\nFinding the sum of 604542 and 6458932\n\n\n7063474\n\n\n\nsource\n\n\nmk_toolres\n\n mk_toolres (r:collections.abc.Mapping,\n             ns:Optional[collections.abc.Mapping]=None, obj:Optional=None)\n\nCreate a tool_result message from response r.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nr\nMapping\n\nTool use request response\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\nobj\nOptional\nNone\nClass to search for tools\n\n\n\n\n\nExported source\ndef mk_toolres(\n    r:abc.Mapping, # Tool use request response\n    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n    obj:Optional=None # Class to search for tools\n    ):\n    \"Create a `tool_result` message from response `r`.\"\n    r = mk_msg(r)\n    tcs = getattr(r, 'tool_calls', [])\n    res = [r]\n    for tc in (tcs or []):\n        func = tc.function\n        cts = str(call_func(func, ns=ns, obj=obj))\n        res.append(mk_msg(str(cts), 'tool', tool_call_id=tc.id, name=func.name))\n    return res\n\n\n\ntr = mk_toolres(r, ns=ns)\ntr\n\nFinding the sum of 604542 and 6458932\n\n\n[ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_lGvaujwxNi5M4Qow7q3MHVRK', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]),\n {'role': 'tool',\n  'content': '7063474',\n  'tool_call_id': 'call_lGvaujwxNi5M4Qow7q3MHVRK',\n  'name': 'sums'}]\n\n\n\nmsgs += tr\n\n\nres = c(msgs, sp=sysp, tools=tools)\nres\n\nThe sum of ( 604542 ) and ( 6458932 ) is ( 7063474 ).\n\n\nid: chatcmpl-9RkngEFS4HLusmCmTu4HjoGjcS0mD\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The sum of \\( 604542 \\) and \\( 6458932 \\) is \\( 7063474 \\).’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401784\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=26, prompt_tokens=136, total_tokens=162)\n\n\n\n\n\nclass Dummy:\n    def sums(\n        self,\n        a:int,  # First thing to sum\n        b:int=1 # Second thing to sum\n    ) -&gt; int: # The sum of the inputs\n        \"Adds a + b.\"\n        print(f\"Finding the sum of {a} and {b}\")\n        return a + b\n\n\ntools = [mk_openai_func(Dummy.sums)]\n\no = Dummy()\nmsgs = mk_toolres(\"I'm Jeremy\")\nr = c(msgs, sp=sysp, tools=tools)\nmsgs += mk_toolres(r, obj=o)\nres = c(msgs, sp=sysp, tools=tools)\nres\n\nHi Jeremy! How can I assist you today?\n\n\nid: chatcmpl-9RknjIS83HvoyhsyP02jOPArmI1Kd\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Hi Jeremy! How can I assist you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401787\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=11, prompt_tokens=116, total_tokens=127)\n\n\n\n\n\nmsgs\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)]\n\n\n\ntools = [mk_openai_func(Dummy.sums)]\n\no = Dummy()\nmsgs = mk_toolres(pr)\nr = c(msgs, sp=sysp, tools=tools)\nmsgs += mk_toolres(r, obj=o)\nres = c(msgs, sp=sysp, tools=tools)\nres\n\nFinding the sum of 604542 and 6458932\n\n\nThe sum of 604542 and 6458932 is 7063474.\n\n\nid: chatcmpl-9RknkRB7hkdbZ0O9nQUDK2pULVYP4\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The sum of 604542 and 6458932 is 7063474.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716401788\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=18, prompt_tokens=142, total_tokens=160)\n\n\n\n\n\nsource\n\n\nmock_tooluse\n\n mock_tooluse (name:str, res, **kwargs)\n\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nThe name of the called function\n\n\nres\n\nThe result of calling the function\n\n\nkwargs\n\n\n\n\n\n\n\nExported source\ndef _mock_id(): return 'call_' + ''.join(choices(ascii_letters+digits, k=24))\n\ndef mock_tooluse(name:str, # The name of the called function\n                 res,  # The result of calling the function\n                 **kwargs): # The arguments to the function\n    \"\"\n    id = _mock_id()\n    func = dict(arguments=json.dumps(kwargs), name=name)\n    tc = dict(id=id, function=func, type='function')\n    req = dict(content=None, role='assistant', tool_calls=[tc])\n    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)\n    return [req,resp]\n\n\nThis function mocks the messages needed to implement tool use, for situations where you want to insert tool use messages into a dialog without actually calling into the model.\n\ntu = mk_tooluse(name='sums', res=7063474, a=604542, b=6458932)\nr = c([mk_msg(pr)]+tu, tools=tools)\nr\n\nThe sum of 604542 and 6458932 is 7,063,474.\n\n\nid: chatcmpl-9Rkv7ncClEoHmJjrHrOb1sj7VZ269\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The sum of 604542 and 6458932 is 7,063,474.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716402245\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=20, prompt_tokens=121, total_tokens=141)",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Cosette’s source",
    "section": "Chat",
    "text": "Chat\n\nsource\n\nChat\n\n Chat (model:Optional[str]=None, cli:Optional[__main__.Client]=None,\n       sp='', tools:Optional[list]=None, tool_choice:Optional[str]=None)\n\nOpenAI chat client.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nOptional\nNone\nModel to use (leave empty if passing cli)\n\n\ncli\nOptional\nNone\nClient to use (leave empty if passing model)\n\n\nsp\nstr\n\nOptional system prompt\n\n\ntools\nOptional\nNone\nList of tools to make available\n\n\ntool_choice\nOptional\nNone\nForced tool choice\n\n\n\n\n\nExported source\nclass Chat:\n    def __init__(self,\n                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n                 sp='', # Optional system prompt\n                 tools:Optional[list]=None,  # List of tools to make available\n                 tool_choice:Optional[str]=None): # Forced tool choice\n        \"OpenAI chat client.\"\n        assert model or cli\n        self.c = (cli or Client(model))\n        self.h,self.sp,self.tools,self.tool_choice = [],sp,tools,tool_choice\n    \n    @property\n    def use(self): return self.c.use\n\n\n\nsp = \"Never mention what tools you use.\"\nchat = Chat(model, sp=sp)\nchat.c.use, chat.h\n\n(In: 0; Out: 0; Total: 0, [])\n\n\n\nsource\n\n\nChat.__call__\n\n Chat.__call__ (pr=None, stream:bool=False,\n                frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN, func\n                tion_call:completion_create_params.FunctionCall|NotGiven=N\n                OT_GIVEN, functions:Iterable[completion_create_params.Func\n                tion]|NotGiven=NOT_GIVEN,\n                logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n                logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n                max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n                n:Optional[int]|NotGiven=NOT_GIVEN,\n                parallel_tool_calls:bool|NotGiven=NOT_GIVEN,\n                presence_penalty:Optional[float]|NotGiven=NOT_GIVEN, respo\n                nse_format:completion_create_params.ResponseFormat|NotGive\n                n=NOT_GIVEN, seed:Optional[int]|NotGiven=NOT_GIVEN, servic\n                e_tier:\"Optional[Literal['auto','default']]|NotGiven\"=NOT_\n                GIVEN,\n                stop:Union[Optional[str],List[str]]|NotGiven=NOT_GIVEN, st\n                ream_options:Optional[ChatCompletionStreamOptionsParam]|No\n                tGiven=NOT_GIVEN,\n                temperature:Optional[float]|NotGiven=NOT_GIVEN, tool_choic\n                e:ChatCompletionToolChoiceOptionParam|NotGiven=NOT_GIVEN, \n                tools:Iterable[ChatCompletionToolParam]|NotGiven=NOT_GIVEN\n                , top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n                top_p:Optional[float]|NotGiven=NOT_GIVEN,\n                user:str|NotGiven=NOT_GIVEN,\n                extra_headers:Headers|None=None,\n                extra_query:Query|None=None, extra_body:Body|None=None,\n                timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nAdd prompt pr to dialog and get a response\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\nNoneType\nNone\nPrompt / message\n\n\nstream\nbool\nFalse\nStream response?\n\n\nfrequency_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nfunction_call\ncompletion_create_params.FunctionCall | NotGiven\nNOT_GIVEN\n\n\n\nfunctions\nIterable[completion_create_params.Function] | NotGiven\nNOT_GIVEN\n\n\n\nlogit_bias\nOptional[Dict[str, int]] | NotGiven\nNOT_GIVEN\n\n\n\nlogprobs\nOptional[bool] | NotGiven\nNOT_GIVEN\n\n\n\nmax_tokens\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nn\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nparallel_tool_calls\nbool | NotGiven\nNOT_GIVEN\n\n\n\npresence_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nresponse_format\ncompletion_create_params.ResponseFormat | NotGiven\nNOT_GIVEN\n\n\n\nseed\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’]] | NotGiven\nNOT_GIVEN\n\n\n\nstop\nUnion[Optional[str], List[str]] | NotGiven\nNOT_GIVEN\n\n\n\nstream_options\nOptional[ChatCompletionStreamOptionsParam] | NotGiven\nNOT_GIVEN\n\n\n\ntemperature\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\ntool_choice\nChatCompletionToolChoiceOptionParam | NotGiven\nNOT_GIVEN\n\n\n\ntools\nIterable[ChatCompletionToolParam] | NotGiven\nNOT_GIVEN\n\n\n\ntop_logprobs\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\ntop_p\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nuser\nstr | NotGiven\nNOT_GIVEN\n\n\n\nextra_headers\nHeaders | None\nNone\n\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Completions.create)\ndef __call__(self:Chat,\n             pr=None,  # Prompt / message\n             stream:bool=False, # Stream response?\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response\"\n    if isinstance(pr,str): pr = pr.strip()\n    if pr: self.h.append(mk_msg(pr))\n    if self.tools: kwargs['tools'] = [mk_openai_func(o) for o in self.tools]\n    if self.tool_choice: kwargs['tool_choice'] = mk_tool_choice(tool_choice)\n    res = self.c(self.h, sp=self.sp, stream=stream, **kwargs)\n    self.h += mk_toolres(res, ns=self.tools, obj=self)\n    return res\n\n\n\nchat(\"I'm Jeremy\")\nchat(\"What's my name?\")\n\nYour name is Jeremy. How can I help you today?\n\n\nid: chatcmpl-9R81BjEI4V2NzVnAcC3wsGs5zVzhx\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Your name is Jeremy. How can I help you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716252705\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=12, prompt_tokens=47, total_tokens=59)\n\n\n\n\n\nchat = Chat(model, sp=sp)\nfor o in chat(\"I'm Jeremy\", stream=True):\n    o = contents(o)\n    if o and isinstance(o, str): print(o, end='')\n\nHi Jeremy! How can I assist you today?\n\n\n\n\nChat tool use\n\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nchat = Chat(model, sp=sp, tools=[sums])\nr = chat(pr)\nr\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: chatcmpl-9R81DDZbcRBWExEW7NU29xar0na3H\nchoices: [Choice(finish_reason=‘tool_calls’, index=0, logprobs=None, message=ChatCompletionMessage(content=None, role=‘assistant’, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=‘call_qwCoELHOZa325UyxdKRAbP8P’, function=Function(arguments=‘{“a”:604542,“b”:6458932}’, name=‘sums’), type=‘function’)]))]\ncreated: 1716252707\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=21, prompt_tokens=90, total_tokens=111)\n\n\n\n\nchat()\n\nThe result of 604542 + 6458932 is 7063474.\n\n\nid: chatcmpl-9R81DrU0V9VmVO9vKA4vtDMXgqZRq\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The result of 604542 + 6458932 is 7063474.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716252707\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=18, prompt_tokens=122, total_tokens=140)",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#images",
    "href": "core.html#images",
    "title": "Cosette’s source",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\n# Image is Cute_dog.jpg from Wikimedia\nfn = Path('samples/puppy.jpg')\ndisplay.Image(filename=fn, width=200)\n\n\n\n\n\n\n\n\n\nimg = fn.read_bytes()\n\n{\n  \"type\": \"image_url\",\n  \"image_url\": {\n    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n  }\n}\n\nsource\n\nimg_msg\n\n img_msg (data:bytes)\n\nConvert image data into an encoded dict\n\n\nExported source\ndef img_msg(data:bytes)-&gt;dict:\n    \"Convert image `data` into an encoded `dict`\"\n    img = base64.b64encode(data).decode(\"utf-8\")\n    mtype = mimetypes.types_map['.'+imghdr.what(None, h=data)]\n    r = {'url': f\"data:{mtype};base64,{img}\"}\n    return {'type': \"image_url\", \"image_url\": r}\n\n\n\nsource\n\n\ntext_msg\n\n text_msg (s:str)\n\nConvert s to a text message\n\n\nExported source\ndef text_msg(s:str)-&gt;dict:\n    \"Convert `s` to a text message\"\n    return {\"type\": \"text\", \"text\": s}\n\n\n\nq = \"In brief, what color flowers are in this image?\"\nmsg = mk_msg([img_msg(img), text_msg(q)])\n\n\nc([msg])\n\nThe flowers in the image are purple.\n\n\nid: chatcmpl-9R7157vVHkk0ZWqLl1bwZkfJPe21H\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The flowers in the image are purple.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716248855\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=8, prompt_tokens=273, total_tokens=281)\n\n\n\n\n\n\nExported source\ndef _mk_content(src):\n    \"Create appropriate content data structure based on type of content\"\n    if isinstance(src,str): return text_msg(src)\n    if isinstance(src,bytes): return img_msg(src)\n    return src\n\n\nThere’s not need to manually choose the type of message, since we figure that out from the data of the source data.\n\n_mk_content('Hi')\n\n{'type': 'text', 'text': 'Hi'}\n\n\n\nsource\n\n\nmk_msg\n\n mk_msg (content, role='user', **kwargs)\n\nHelper to create a dict appropriate for a message. kwargs are added as key/value pairs to the message\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\n\n\nA string, list, or dict containing the contents of the message\n\n\nrole\nstr\nuser\nMust be ‘user’ or ‘assistant’\n\n\nkwargs\n\n\n\n\n\n\n\nc([mk_msg([img, q])])\n\nThe flowers in this image are purple.\n\n\nid: chatcmpl-9R76D0a9kD3Hy2IuBScq0VhqyFcaz\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The flowers in this image are purple.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716249173\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=8, prompt_tokens=273, total_tokens=281)",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "toolloop.html",
    "href": "toolloop.html",
    "title": "Tool loop",
    "section": "",
    "text": "model = models[0]\n\n\norders = {\n    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n\ncustomers = {\n    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n               orders=[orders['O1'], orders['O2']]),\n    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n               orders=[orders['O3']])\n}\n\n\ndef get_customer_info(\n    customer_id:str # ID of the customer\n): # Customer's name, email, phone number, and list of orders\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_order_details(\n    order_id:str # ID of the order\n): # Order's ID, product name, quantity, price, and order status\n    \"Retrieves the details of a specific order based on the order ID\"\n    print(f'- Retrieving order {order_id}')\n    return orders.get(order_id, \"Order not found\")\n\ndef cancel_order(\n    order_id:str # ID of the order to cancel\n)-&gt;bool: # True if the cancellation is successful\n    \"Cancels an order based on the provided order ID\"\n    print(f'- Cancelling order {order_id}')\n    if order_id not in orders: return False\n    orders[order_id]['status'] = 'Cancelled'\n    return True\n\n\ntools = [get_customer_info, get_order_details, cancel_order]\nchat = Chat(model, tools=tools)\n\n\nr = chat('Can you tell me the email address for customer C2?')\n\n- Retrieving customer C2\n\n\n\nchoice = r.choices[0]\nprint(choice.finish_reason)\nchoice\n\ntool_calls\n\n\nChoice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_BYev4ExQk899v9yjLERyDGSc', function=Function(arguments='{\"customer_id\":\"C2\"}', name='get_customer_info'), type='function')]))\n\n\n\nr = chat()\nr\n\nThe email address for customer C2 (Jane Smith) is jane@example.com.\n\n\nid: chatcmpl-9R81d5i8pBSIRg5B7erJcF8t5BzKw\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The email address for customer C2 (Jane Smith) is jane@example.com.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716252733\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=17, prompt_tokens=251, total_tokens=268)\n\n\n\n\n\nchat = Chat(model, tools=tools)\nr = chat('Please cancel all orders for customer C1 for me.')\nprint(r.choices[0].finish_reason)\nfind_block(r)\n\n- Retrieving customer C1\ntool_calls\n\n\nChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_lENp0aVeTJ7W0q8SRgC7h5s9', function=Function(arguments='{\"customer_id\":\"C1\"}', name='get_customer_info'), type='function')])\n\n\n\nsource\n\nChat.toolloop\n\n Chat.toolloop (pr, max_steps=10, trace_func:Optional[&lt;built-\n                infunctioncallable&gt;]=None, cont_func:Optional[&lt;built-\n                infunctioncallable&gt;]=&lt;function noop&gt;,\n                frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN, func\n                tion_call:completion_create_params.FunctionCall|NotGiven=N\n                OT_GIVEN, functions:Iterable[completion_create_params.Func\n                tion]|NotGiven=NOT_GIVEN,\n                logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n                logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n                max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n                n:Optional[int]|NotGiven=NOT_GIVEN,\n                parallel_tool_calls:bool|NotGiven=NOT_GIVEN,\n                presence_penalty:Optional[float]|NotGiven=NOT_GIVEN, respo\n                nse_format:completion_create_params.ResponseFormat|NotGive\n                n=NOT_GIVEN, seed:Optional[int]|NotGiven=NOT_GIVEN, servic\n                e_tier:\"Optional[Literal['auto','default']]|NotGiven\"=NOT_\n                GIVEN,\n                stop:Union[Optional[str],List[str]]|NotGiven=NOT_GIVEN, st\n                ream:Optional[Literal[False]]|Literal[True]|NotGiven=NOT_G\n                IVEN, stream_options:Optional[ChatCompletionStreamOptionsP\n                aram]|NotGiven=NOT_GIVEN,\n                temperature:Optional[float]|NotGiven=NOT_GIVEN, tool_choic\n                e:ChatCompletionToolChoiceOptionParam|NotGiven=NOT_GIVEN, \n                tools:Iterable[ChatCompletionToolParam]|NotGiven=NOT_GIVEN\n                , top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n                top_p:Optional[float]|NotGiven=NOT_GIVEN,\n                user:str|NotGiven=NOT_GIVEN,\n                extra_headers:Headers|None=None,\n                extra_query:Query|None=None, extra_body:Body|None=None,\n                timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nAdd prompt pr to dialog and get a response from the model, automatically following up with tool_use messages\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\n\n\nPrompt to pass to model\n\n\nmax_steps\nint\n10\nMaximum number of tool requests to loop through\n\n\ntrace_func\nOptional\nNone\nFunction to trace tool use steps (e.g print)\n\n\ncont_func\nOptional\nnoop\nFunction that stops loop if returns False\n\n\nfrequency_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nfunction_call\ncompletion_create_params.FunctionCall | NotGiven\nNOT_GIVEN\n\n\n\nfunctions\nIterable[completion_create_params.Function] | NotGiven\nNOT_GIVEN\n\n\n\nlogit_bias\nOptional[Dict[str, int]] | NotGiven\nNOT_GIVEN\n\n\n\nlogprobs\nOptional[bool] | NotGiven\nNOT_GIVEN\n\n\n\nmax_tokens\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nn\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nparallel_tool_calls\nbool | NotGiven\nNOT_GIVEN\n\n\n\npresence_penalty\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nresponse_format\ncompletion_create_params.ResponseFormat | NotGiven\nNOT_GIVEN\n\n\n\nseed\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’]] | NotGiven\nNOT_GIVEN\n\n\n\nstop\nUnion[Optional[str], List[str]] | NotGiven\nNOT_GIVEN\n\n\n\nstream\nOptional[Literal[False]] | Literal[True] | NotGiven\nNOT_GIVEN\n\n\n\nstream_options\nOptional[ChatCompletionStreamOptionsParam] | NotGiven\nNOT_GIVEN\n\n\n\ntemperature\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\ntool_choice\nChatCompletionToolChoiceOptionParam | NotGiven\nNOT_GIVEN\n\n\n\ntools\nIterable[ChatCompletionToolParam] | NotGiven\nNOT_GIVEN\n\n\n\ntop_logprobs\nOptional[int] | NotGiven\nNOT_GIVEN\n\n\n\ntop_p\nOptional[float] | NotGiven\nNOT_GIVEN\n\n\n\nuser\nstr | NotGiven\nNOT_GIVEN\n\n\n\nextra_headers\nHeaders | None\nNone\n\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Completions.create)\ndef toolloop(self:Chat,\n             pr, # Prompt to pass to model\n             max_steps=10, # Maximum number of tool requests to loop through\n             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response from the model, automatically following up with `tool_use` messages\"\n    r = self(pr, **kwargs)\n    for i in range(max_steps):\n        ch = r.choices[0]\n        if ch.finish_reason!='tool_calls': break\n        if trace_func: trace_func(r)\n        r = self(**kwargs)\n        if not (cont_func or noop)(self.h[-2]): break\n    if trace_func: trace_func(r)\n    return r\n\n\n\nchat = Chat(model, tools=tools)\nr = chat.toolloop('Please cancel all orders for customer C1 for me.', trace_func=print)\nr\n\n- Retrieving customer C1\nChatCompletion(id='chatcmpl-9R81fvatrbbrAuUjZRTPKWgntsRCx', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Yss1rLc1tU2wDkWPMGSGgdor', function=Function(arguments='{\"customer_id\":\"C1\"}', name='get_customer_info'), type='function')]))], created=1716252735, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_729ea513f7', usage=In: 157; Out: 17; Total: 174)\n- Cancelling order O1\n- Cancelling order O2\nChatCompletion(id='chatcmpl-9R81gjtdy8L3cUI4o46MDeeOdaaRb', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4AIzkbEYXTGNqd6uMsPLuVEB', function=Function(arguments='{\"order_id\": \"O1\"}', name='cancel_order'), type='function'), ChatCompletionMessageToolCall(id='call_Px4E3w1qEJZCIrU5ymf6qsNt', function=Function(arguments='{\"order_id\": \"O2\"}', name='cancel_order'), type='function')]))], created=1716252736, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_729ea513f7', usage=In: 283; Out: 48; Total: 331)\nChatCompletion(id='chatcmpl-9R81hQpOI0m1ExNidpdMrIiMx78pd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Both orders for customer C1 have been successfully canceled.', role='assistant', function_call=None, tool_calls=None))], created=1716252737, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_729ea513f7', usage=In: 347; Out: 12; Total: 359)\n\n\nBoth orders for customer C1 have been successfully canceled.\n\n\nid: chatcmpl-9R81hQpOI0m1ExNidpdMrIiMx78pd\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Both orders for customer C1 have been successfully canceled.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716252737\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=12, prompt_tokens=347, total_tokens=359)\n\n\n\n\n\nchat.toolloop('What is the status of order O2?')\n\n- Retrieving order O2\n\n\nThe status of order O2 is “Cancelled”.\n\n\nid: chatcmpl-9R81inXsiw5xzoux1xw5Z7bBoPsuN\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The status of order O2 is “Cancelled”.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716252738\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=11, prompt_tokens=436, total_tokens=447)",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "cosette",
    "section": "Getting started",
    "text": "Getting started\nOpenAI’s Python SDK will automatically be installed with Cosette, if you don’t already have it.\n\nfrom cosette import *\n\nCosette only exports the symbols that are needed to use the library, so you can use import * to import them. Alternatively, just use:\nimport cosette\n…and then add the prefix cosette. to any usages of the module.\nCosette provides models, which is a list of models currently available from the SDK.\n\nmodels\n\n('gpt-4o',\n 'gpt-4-turbo',\n 'gpt-4',\n 'gpt-4-32k',\n 'gpt-3.5-turbo',\n 'gpt-3.5-turbo-instruct')\n\n\nFor these examples, we’ll use GPT-4o.\n\nmodel = models[0]",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "cosette",
    "section": "Chat",
    "text": "Chat\nThe main interface to Cosette is the Chat class, which provides a stateful interface to the models:\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat(\"I'm Jeremy\")\n\nHi Jeremy! How can I assist you today?\n\n\nid: chatcmpl-9R8Z0uRHgWl7XaV6yJtahVDyDTzMZ\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Hi Jeremy! How can I assist you today?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254802\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=10, prompt_tokens=21, total_tokens=31)\n\n\n\n\n\nr = chat(\"What's my name?\")\nr\n\nYour name is Jeremy. How can I assist you further?\n\n\nid: chatcmpl-9R8Z1c76TFqYFYjyON08CbkAmjerN\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘Your name is Jeremy. How can I assist you further?’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254803\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=12, prompt_tokens=43, total_tokens=55)\n\n\n\n\nAs you see above, displaying the results of a call in a notebook shows just the message contents, with the other details hidden behind a collapsible section. Alternatively you can print the details:\n\nprint(r)\n\nChatCompletion(id='chatcmpl-9R8Z1c76TFqYFYjyON08CbkAmjerN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Your name is Jeremy. How can I assist you further?', role='assistant', function_call=None, tool_calls=None))], created=1716254803, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_729ea513f7', usage=In: 43; Out: 12; Total: 55)\n\n\nYou can use stream=True to stream the results as soon as they arrive (although you will only see the gradual generation if you execute the notebook yourself, of course!)\n\nfor o in chat(\"What's your name?\", stream=True): print(o, end='')\n\nI don't have a personal name, but you can call me Assistant. How can I help you today, Jeremy?",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#tool-use",
    "href": "index.html#tool-use",
    "title": "cosette",
    "section": "Tool use",
    "text": "Tool use\nTool use lets the model use external tools.\nWe use docments to make defining Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. As an example we’ll write a simple function that adds numbers together, and will tell us when it’s being called:\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\nSometimes the model will say something like “according to the sums tool the answer is” – generally we’d rather it just tells the user the answer, so we can use a system prompt to help with this:\n\nsp = \"Never mention what tools you use.\"\n\nWe’ll get the model to add up some long numbers:\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\nTo use tools, pass a list of them to Chat:\n\nchat = Chat(model, sp=sp, tools=[sums])\n\nNow when we call that with our prompt, the model doesn’t return the answer, but instead returns a tool_use message, which means we have to call the named tool with the provided parameters:\n\nr = chat(pr)\nr\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: chatcmpl-9R8Z2JNenseQyQoseIs8XNImmy2Bo\nchoices: [Choice(finish_reason=‘tool_calls’, index=0, logprobs=None, message=ChatCompletionMessage(content=None, role=‘assistant’, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=‘call_HV4yaZEY1OYK1zYouAcVwfZK’, function=Function(arguments=‘{“a”:604542,“b”:6458932}’, name=‘sums’), type=‘function’)]))]\ncreated: 1716254804\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=21, prompt_tokens=96, total_tokens=117)\n\n\n\nCosette handles all that for us – we just have to pass along the message, and it all happens automatically:\n\nchat()\n\nThe sum of 604542 and 6458932 is 7063474.\n\n\nid: chatcmpl-9R8Z4CrFU3zd71acZzdCsQFQDHxp9\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The sum of 604542 and 6458932 is 7063474.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254806\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_729ea513f7\nusage: CompletionUsage(completion_tokens=18, prompt_tokens=128, total_tokens=146)\n\n\n\n\nYou can see how many tokens have been used at any time by checking the use property.\n\nchat.use\n\nIn: 224; Out: 39; Total: 263\n\n\n\nTool loop\nWe can do everything needed to use tools in a single step, by using Chat.toolloop. This can even call multiple tools as needed solve a problem. For example, let’s define a tool to handle multiplication:\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\nNow with a single call we can calculate (a+b)*2 – by passing show_trace we can see each response from the model in the process:\n\nchat = Chat(model, sp=sp, tools=[sums,mults])\npr = f'Calculate ({a}+{b})*2'\npr\n\n'Calculate (604542+6458932)*2'\n\n\n\ndef pchoice(r): print(r.choices[0])\n\n\nr = chat.toolloop(pr, trace_func=pchoice)\n\nFinding the sum of 604542 and 6458932\nFinding the product of 2 and 1\nChoice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_OfypQBQoAuIUksucevaxwH5Z', function=Function(arguments='{\"a\": 604542, \"b\": 6458932}', name='sums'), type='function'), ChatCompletionMessageToolCall(id='call_yKAL5o96cDef83OFJhDB21MM', function=Function(arguments='{\"a\": 2}', name='mults'), type='function')]))\nFinding the product of 7063474 and 2\nChoice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Ffye7Tf65CjVjwwx8Sp8031i', function=Function(arguments='{\"a\":7063474,\"b\":2}', name='mults'), type='function')]))\nChoice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The result of \\\\((604542 + 6458932) \\\\times 2\\\\) is 14,126,948.', role='assistant', function_call=None, tool_calls=None))\n\n\nOpenAI uses special tags for math equations, which we can replace using wrap_latex:\n\nwrap_latex(contents(r))\n\nThe result of \\((604542 + 6458932) \\times 2\\) is 14,126,948.",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#images",
    "href": "index.html#images",
    "title": "cosette",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\nfn = Path('samples/puppy.jpg')\ndisplay.Image(filename=fn, width=200)\n\n\n\n\n\n\n\n\nWe create a Chat object as before:\n\nchat = Chat(model)\n\nClaudia expects images as a list of bytes, so we read in the file:\n\nimg = fn.read_bytes()\n\nPrompts to Claudia can be lists, containing text, images, or both, eg:\n\nchat([img, \"In brief, what color flowers are in this image?\"])\n\nThe flowers in the image are purple.\n\n\nid: chatcmpl-9R8Vqpx62OezZDjAt3SIfnjMpH3I8\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The flowers in the image are purple.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254606\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=8, prompt_tokens=273, total_tokens=281)\n\n\n\n\nThe image is included as input tokens.\n\nchat.use\n\nIn: 273; Out: 8; Total: 281\n\n\nAlternatively, Cosette supports creating a multi-stage chat with separate image and text prompts. For instance, you can pass just the image as the initial prompt (in which case the model will make some general comments about what it sees), and then follow up with questions in additional prompts:\n\nchat = Chat(model)\nchat(img)\n\nWhat an adorable puppy! This puppy has a white and light brown coat and is lying on green grass next to some purple flowers. Puppies like this are commonly seen from breeds such as Cavalier King Charles Spaniels, though without more context, it’s difficult to identify the breed precisely. It looks very playful and cute!\n\n\nid: chatcmpl-9R8VsAnTWr9k1DShC7mZsnhRtqxRA\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=“What an adorable puppy! This puppy has a white and light brown coat and is lying on green grass next to some purple flowers. Puppies like this are commonly seen from breeds such as Cavalier King Charles Spaniels, though without more context, it’s difficult to identify the breed precisely. It looks very playful and cute!”, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254608\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=63, prompt_tokens=262, total_tokens=325)\n\n\n\n\n\nchat('What direction is the puppy facing?')\n\nThe puppy is facing slightly to the right of the camera, with its head turned towards the viewer. Its body is positioned in such a way that suggests it is laying down or resting on the grass.\n\n\nid: chatcmpl-9R8VuzGIABwg341oOHMXbGGa7daya\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The puppy is facing slightly to the right of the camera, with its head turned towards the viewer. Its body is positioned in such a way that suggests it is laying down or resting on the grass.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254610\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=40, prompt_tokens=340, total_tokens=380)\n\n\n\n\n\nchat('What color is it?')\n\nThe puppy has a predominantly white coat with light brown patches, particularly around its ears and eyes. This coloration is commonly seen in certain breeds, such as the Cavalier King Charles Spaniel.\n\n\nid: chatcmpl-9R8Vwtlu6aDEGQ8O7bZFk8rfT9FGL\nchoices: [Choice(finish_reason=‘stop’, index=0, logprobs=None, message=ChatCompletionMessage(content=‘The puppy has a predominantly white coat with light brown patches, particularly around its ears and eyes. This coloration is commonly seen in certain breeds, such as the Cavalier King Charles Spaniel.’, role=‘assistant’, function_call=None, tool_calls=None))]\ncreated: 1716254612\nmodel: gpt-4o-2024-05-13\nobject: chat.completion\nsystem_fingerprint: fp_927397958d\nusage: CompletionUsage(completion_tokens=38, prompt_tokens=393, total_tokens=431)\n\n\n\n\nNote that the image is passed in again for every input in the dialog, so that number of input tokens increases quickly with this kind of chat.\n\nchat.use\n\nIn: 995; Out: 141; Total: 1136",
    "crumbs": [
      "cosette"
    ]
  }
]